\chapter{Vocabulary}\label{app:vocabulary}
\todo{This is a placeholder for the vocabulary appendix.}
In this appendix, we will present and explain some specific words and terms used in the thesis.

\textbf{EEG} - Electroencephalography is a method to record electrical activity of the brain. It is a non-invasive method that uses electrodes placed on the scalp to detect electrical activity in the brain.

\textbf{Motor Imagery} - Motor imagery is a mental process in which an individual imagines themselves performing a specific motor task without actually performing the task physically. It is used in brain-computer interfaces to control devices using brain signals.

\textbf{Brain-Computer Interface (BCI)} - A brain-computer interface is a system that allows direct communication between the brain and an external device, such as a computer or a robotic arm. BCIs can be used to control devices using brain signals.

\textbf{Machine Learning} - Machine learning is a field of artificial intelligence that uses statistical techniques to give computers the ability to learn from data without being explicitly programmed. Machine learning algorithms can be used to analyze and interpret brain signals in BCIs.

\textbf{Deep Learning} - Deep learning is a subfield of machine learning that uses artificial neural networks to model and interpret complex patterns in data. Deep learning algorithms can be used to analyze and interpret brain signals in BCIs.

\textbf{Convolutional Neural Network (CNN)} - A convolutional neural network is a type of deep learning algorithm that is commonly used for image recognition tasks. CNNs are composed of multiple layers of neurons that apply convolutional filters to input data to extract features.

\textbf{Recurrent Neural Network (RNN)} - A recurrent neural network is a type of deep learning algorithm that is commonly used for sequence prediction tasks. RNNs have connections between neurons that form loops, allowing them to process sequences of data.

\textbf{Long Short-Term Memory (LSTM)} - Long short-term memory is a type of recurrent neural network that is designed to capture long-term dependencies in sequential data. LSTMs have memory cells that can store information over long periods of time.

\textbf{Electrode} - An electrode is a conductor that is used to make electrical contact with a non-metallic part of a circuit, such as the skin. In EEG, electrodes are placed on the scalp to detect electrical activity in the brain.

\textbf{Feature Extraction} - Feature extraction is the process of transforming raw data into a set of features that can be used as input to a machine learning algorithm. In EEG signal processing, feature extraction is used to extract relevant information from brain signals.

\textbf{Classification} - Classification is a machine learning task that involves assigning a label to an input data point based on its features. In BCIs, classification algorithms are used to predict the intended movement or action from brain signals.

\textbf{Accuracy} - Accuracy is a measure of the performance of a classification algorithm that measures the proportion of correctly classified data points. It is calculated as the number of correct predictions divided by the total number of predictions.

\textbf{Precision} - Precision is a measure of the performance of a classification algorithm that measures the proportion of correctly classified positive data points among all data points predicted as positive. It is calculated as the number of true positive predictions divided by the sum of true positive and false positive predictions.

\textbf{Recall} - Recall is a measure of the performance of a classification algorithm that measures the proportion of correctly classified positive data points among all actual positive data points. It is calculated as the number of true positive predictions divided by the sum of true positive and false negative predictions.

\textbf{F1 Score} - The F1 score is a measure of the performance of a classification algorithm that combines precision and recall into a single metric. It is calculated as the harmonic mean of precision and recall.

\textbf{Cross-Validation} - Cross-validation is a technique used to evaluate the performance of a machine learning algorithm by splitting the data into multiple subsets, training the algorithm on one subset, and testing it on the remaining subsets. Cross-validation helps to assess the generalization performance of the algorithm.

\textbf{Hyperparameter} - A hyperparameter is a parameter of a machine learning algorithm that is set before the training process begins. Hyperparameters control the behavior of the algorithm and can be tuned to optimize its performance.

\textbf{Grid Search} - Grid search is a technique used to find the optimal hyperparameters of a machine learning algorithm by exhaustively searching through a specified set of hyperparameter values. Grid search evaluates the performance of the algorithm for each combination of hyperparameters and selects the best one.

\textbf{Random Search} - Random search is a technique used to find the optimal hyperparameters of a machine learning algorithm by randomly sampling hyperparameter values from a specified distribution. Random search evaluates the performance of the algorithm for a random subset of hyperparameter values and selects the best one.

\textbf{Transfer Learning} - Transfer learning is a machine learning technique that involves using knowledge gained from one task to improve the performance of another related task. In BCIs, transfer learning can be used to leverage pre-trained models on similar datasets to improve the performance of classification algorithms.

\textbf{Fine-Tuning} - Fine-tuning is a transfer learning technique that involves retraining a pre-trained model on a new dataset to adapt it to a specific task. Fine-tuning allows the model to learn task-specific features while retaining the knowledge gained from the pre-training.

\textbf{Data Augmentation} - Data augmentation is a technique used to increase the size of a dataset by applying transformations to the original data. In BCIs, data augmentation can be used to generate additional training samples from existing data to improve the performance of classification algorithms.

\textbf{Regularization} - Regularization is a technique used to prevent overfitting in machine learning algorithms by adding a penalty term to the loss function. Regularization helps to reduce the complexity of the model and improve its generalization performance.

\textbf{Dropout} - Dropout is a regularization technique used in deep learning algorithms to prevent overfitting by randomly setting a fraction of the neurons to zero during training. Dropout helps to reduce the interdependence between neurons and improve the generalization performance of the model.

\textbf{Batch Normalization} - Batch normalization is a technique used in deep learning algorithms to improve the training speed and stability of the model. Batch normalization normalizes the input data to each layer of the network, reducing the internal covariate shift and improving the convergence of the model.

\textbf{Loss Function} - A loss function is a function that measures the error between the predicted output of a machine learning algorithm and the true output. The loss function is used to optimize the parameters of the model during training.

\textbf{Gradient Descent} - Gradient descent is an optimization algorithm used to minimize the loss function of a machine learning model by iteratively updating the parameters in the direction of the negative gradient. Gradient descent helps to find the optimal parameters of the model that minimize the error.

\textbf{Backpropagation} - Backpropagation is an algorithm used to compute the gradients of the loss function with respect to the parameters of a neural network. Backpropagation is used in conjunction with gradient descent to update the parameters of the model during training.

\textbf{Epoch} - An epoch is a single pass through the entire training dataset during the training of a machine learning algorithm. Training a model for multiple epochs allows it to learn the underlying patterns in the data and improve its performance.

\textbf{Batch Size} - The batch size is the number of data points used to update the parameters of a machine learning model during training. Training the model in batches helps to reduce the memory requirements and speed up the training process.

\textbf{Learning Rate} - The learning rate is a hyperparameter of a machine learning algorithm that controls the size of the step taken during optimization. A high learning rate can lead to faster convergence but may cause the model to overshoot the optimal solution, while a low learning rate can lead to slow convergence but may help the model to find a better solution.

\textbf{Activation Function} - An activation function is a non-linear function that is applied to the output of a neuron in a neural network. Activation functions introduce non-linearity to the model, allowing it to learn complex patterns in the data.

\textbf{Rectified Linear Unit (ReLU)} - The rectified linear unit is an activation function commonly used in deep learning algorithms. ReLU sets the output of a neuron to zero for negative inputs and passes positive inputs unchanged, introducing non-linearity to the model.

\textbf{eLU} - The exponential linear unit is an activation function that is similar to ReLU but allows negative inputs to pass through with a small slope. eLU helps to prevent the dying ReLU problem and improve the convergence of the model.

\textbf{Softmax} - Softmax is an activation function commonly used in classification algorithms to convert the output of a neural network into a probability distribution over multiple classes. Softmax normalizes the output values to sum to one, allowing the model to make probabilistic predictions.

\textbf{Loss Function} - A loss function is a function that measures the error between the predicted output of a machine learning algorithm and the true output. The loss function is used to optimize the parameters of the model during training.

\textbf{Cross-Entropy Loss} - Cross-entropy loss is a loss function commonly used in classification algorithms to measure the difference between the predicted probability distribution and the true distribution of the data. Cross-entropy loss penalizes the model for making incorrect predictions and encourages it to make more confident predictions.

\textbf{Mean Squared Error (MSE)} - Mean squared error is a loss function commonly used in regression algorithms to measure the average squared difference between the predicted output and the true output. MSE penalizes the model for large errors and encourages it to make accurate predictions.

\textbf{Mean Absolute Error (MAE)} - Mean absolute error is a loss function commonly used in regression algorithms to measure the average absolute difference between the predicted output and the true output. MAE is less sensitive to outliers than MSE and provides a more robust measure of error.

\textbf{Overfitting} - Overfitting is a common problem in machine learning algorithms that occurs when the model learns the noise in the training data rather than the underlying patterns. Overfitting leads to poor generalization performance and high error on unseen data.

\textbf{Underfitting} - Underfitting is a common problem in machine learning algorithms that occurs when the model is too simple to capture the underlying patterns in the data. Underfitting leads to poor performance on both the training and test data.

\textbf{Bias} - Bias is a measure of the error introduced by approximating a real-world problem with a simplified model. Bias is the difference between the average prediction of the model and the true value of the data.

\textbf{Variance} - Variance is a measure of the sensitivity of a model to the fluctuations in the training data. Variance is the variability of the model's predictions for a given data point.

\textbf{Bias-Variance Tradeoff} - The bias-variance tradeoff is a fundamental concept in machine learning that describes the relationship between the bias and variance of a model. Increasing the complexity of the model reduces bias but increases variance, while decreasing the complexity of the model reduces variance but increases bias.

\textbf{Feature Selection} - Feature selection is the process of selecting a subset of relevant features from the original set of features to improve the performance of a machine learning algorithm. Feature selection helps to reduce the dimensionality of the data and remove irrelevant or redundant features.

\textbf{Principal Component Analysis (PCA)} - Principal component analysis is a dimensionality reduction technique that transforms the original features of the data into a new set of orthogonal features called principal components. PCA helps to reduce the dimensionality of the data while preserving the most important information.

\textbf{Independent Component Analysis (ICA)} - Independent component analysis is a blind source separation technique that separates a multivariate signal into additive subcomponents that are statistically independent. ICA is commonly used in EEG signal processing to separate brain signals from noise.

\textbf{Support Vector Machine (SVM)} - A support vector machine is a supervised machine learning algorithm that is commonly used for classification tasks. SVM finds the optimal hyperplane that separates the data into different classes by maximizing the margin between the classes.

\textbf{Kernel Trick} - The kernel trick is a technique used in support vector machines to transform the input data into a higher-dimensional space to make it linearly separable. The kernel trick allows SVMs to learn complex patterns in the data without explicitly computing the transformed features.

\textbf{K-Nearest Neighbors (KNN)} - K-nearest neighbors is a supervised machine learning algorithm that classifies data points based on the majority vote of their k nearest neighbors. KNN is a non-parametric algorithm that does not make any assumptions about the underlying distribution of the data.

\textbf{Decision Tree} - A decision tree is a supervised machine learning algorithm that uses a tree-like structure to model the decision-making process. Decision trees split the data into subsets based on the value of the features and make predictions based on the majority class of the subsets.

\textbf{Random Forest} - A random forest is an ensemble learning algorithm that combines multiple decision trees to improve the performance of the model. Random forests train each tree on a random subset of the data and make predictions based on the majority vote of the trees.

\textbf{Gradient Boosting} - Gradient boosting is an ensemble learning algorithm that combines multiple weak learners to create a strong learner. Gradient boosting trains each weak learner on the residual errors of the previous learners and makes predictions by summing the predictions of all learners.

\textbf{AdaBoost} - AdaBoost is an ensemble learning algorithm that combines multiple weak learners to create a strong learner. AdaBoost assigns weights to each data point and trains each weak learner on the misclassified data points of the previous learners.

\textbf{XGBoost} - XGBoost is an optimized implementation of gradient boosting that is designed for speed and performance. XGBoost uses a regularized objective function and parallel processing to improve the training speed and accuracy of the model.

\textbf{LightGBM} - LightGBM is a gradient boosting framework that is designed for efficiency and scalability. LightGBM uses a histogram-based algorithm to split the data and reduce the memory usage and training time of the model.

\textbf{CatBoost} - CatBoost is a gradient boosting library that is designed for categorical data. CatBoost uses an efficient algorithm to handle categorical features and improve the performance of the model.

\textbf{Hyperparameter Tuning} - Hyperparameter tuning is the process of finding the optimal hyperparameters of a machine learning algorithm to improve its performance. Hyperparameter tuning involves searching through a specified set of hyperparameter values and selecting the best one based on a performance metric.

\textbf{Adam Optimizer} - Adam is an optimization algorithm commonly used in deep learning algorithms to update the parameters of the model. Adam adapts the learning rate of each parameter based on the first and second moments of the gradients, allowing it to converge faster and more efficiently.

\textbf{Stochastic Gradient Descent (SGD)} - Stochastic gradient descent is an optimization algorithm commonly used in machine learning algorithms to update the parameters of the model. SGD updates the parameters based on the gradient of the loss function computed on a random subset of the data, allowing it to converge faster and more efficiently.